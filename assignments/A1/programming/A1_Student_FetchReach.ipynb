{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "stzlSyGe8TOU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Due Date**\n",
        "2/13/2025 at 11:59PM EST\n",
        "\n",
        "# **Introduction**\n",
        "\n",
        "Welcome to Assignment 2 of CS 4756/5756. In this assignment, you will train an agent using demonstrations from an expert. Concretely, you will:\n",
        "* Implement behavior cloning (BC)\n",
        "* **CS 5756 Only:** Implement dataset aggregation (DAgger)\n",
        "\n",
        "You will use the Fetch agent for this assignment (Reach task), which is part of Gymasium Robotics' Mujoco Environments. Refer to the Gym website for more details about the [Fetch Reach environment](https://robotics.farama.org/envs/fetch/reach/).\n",
        "\n",
        "\n",
        "Please read through the following paragraphs carefully, as they will apply to this and all future assignments.\n",
        "\n",
        "**Getting Started:** This assignment should be completed in [Google Colab](https://colab.research.google.com/). In order to visualize the environment rollouts, you may switch your runtime to the T4 GPU - however, it is not required for completion of the assignment.\n",
        "\n",
        "**Evaluation:**\n",
        "Your code will be tested for correctness and, for certain assignments, speed. For this particular assignment, performance results will not be harshly graded (although we provide approximate expected reward numbers as lower bounds, you are not expected to replicate them exactly); however, it will be important to make an effort to justify your approach which led to the obtained results. Please remember that all assignments should be completed individually.\n",
        "\n",
        "**Academic Integrity:** We will be checking your code against other submissions in the class for logical redundancy. If you copy someone else’s code and submit it with minor changes, we will know. These cheat detectors are quite hard to fool, so please don’t try. We trust you all to submit your own work only; please don’t let us down. If you do, we will pursue the strongest consequences available to us.\n",
        "\n",
        "**Getting Help:** The [Resources](https://www.cs.cornell.edu/courses/cs4756/2024sp/#resources) section on the course website is your friend! If you ever feel stuck in these projects, please feel free to avail yourself to office hours and Edstem! If you are unable to make any of the office hours listed, please let TAs know and we will be happy to assist. If you need a refresher for PyTorch, please see this [60 minute blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)! For Numpy, please see the quickstart [here](https://numpy.org/doc/stable/user/quickstart.html) and full API [here](https://numpy.org/doc/stable/reference/).\n"
      ],
      "metadata": {
        "id": "mX2Oj9PL7zxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation and Setup"
      ],
      "metadata": {
        "id": "stzlSyGe8TOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"cython<3\"\n",
        "!git clone https://github.com/Farama-Foundation/Gymnasium-Robotics.git\n",
        "!pip install -e Gymnasium-Robotics"
      ],
      "metadata": {
        "id": "4BBjyusBO-pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Include this at the top of your colab code\n",
        "import os\n",
        "!rm .mujoco_setup_complete\n",
        "if not os.path.exists('.mujoco_setup_complete'):\n",
        "  # Get the prereqs\n",
        "  !apt-get -qq update\n",
        "  !apt-get -qq install -y libosmesa6-dev libgl1-mesa-glx libglfw3 libgl1-mesa-dev libglew-dev patchelf\n",
        "  # Get Mujoco\n",
        "  !rm -rf ~/.mujoco\n",
        "  !mkdir ~/.mujoco\n",
        "  !wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O mujoco.tar.gz\n",
        "  !tar -zxf mujoco.tar.gz -C \"$HOME/.mujoco\"\n",
        "  !rm mujoco.tar.gz\n",
        "  # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
        "  !echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/.mujoco/mujoco210/bin' >> ~/.bashrc\n",
        "  !echo 'export LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libGLEW.so' >> ~/.bashrc\n",
        "  # THE ANNOYING ONE, FORCE IT INTO LDCONFIG SO WE ACTUALLY GET ACCESS TO IT THIS SESSION\n",
        "  !echo \"/root/.mujoco/mujoco210/bin\" > /etc/ld.so.conf.d/mujoco_ld_lib_path.conf\n",
        "  !ldconfig\n",
        "  # Install Mujoco-py\n",
        "  !pip3 install -U 'mujoco-py<2.2,>=2.1'\n",
        "  # run once\n",
        "  !touch .mujoco_setup_complete\n",
        "\n",
        "\n",
        "try:\n",
        "  if _mujoco_run_once:\n",
        "    pass\n",
        "except NameError:\n",
        "  _mujoco_run_once = False\n",
        "if not _mujoco_run_once:\n",
        "  # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
        "  try:\n",
        "    os.environ['LD_LIBRARY_PATH']=os.environ['LD_LIBRARY_PATH'] + ':/root/.mujoco/mujoco210/bin'\n",
        "  except KeyError:\n",
        "    os.environ['LD_LIBRARY_PATH']='/root/.mujoco/mujoco210/bin'\n",
        "  try:\n",
        "    os.environ['LD_PRELOAD']=os.environ['LD_PRELOAD'] + ':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
        "  except KeyError:\n",
        "    os.environ['LD_PRELOAD']='/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
        "  # presetup so we don't see output on first env initialization\n",
        "  import mujoco_py\n",
        "  _mujoco_run_once = True"
      ],
      "metadata": {
        "id": "uKHH-XGQW5aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available\")\n",
        "    !pip install mujoco\n",
        "\n",
        "    # Set up GPU rendering.\n",
        "    from google.colab import files\n",
        "    import distutils.util\n",
        "    import os\n",
        "    import subprocess\n",
        "    if subprocess.run('nvidia-smi').returncode:\n",
        "        raise RuntimeError(\n",
        "            'Cannot communicate with GPU. '\n",
        "            'Make sure you are using a GPU Colab runtime. '\n",
        "            'Go to the Runtime menu and select Choose runtime type.')\n",
        "\n",
        "    # Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
        "    # This is usually installed as part of an Nvidia driver package, but the Colab\n",
        "    # kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
        "    # (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
        "    NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
        "    if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
        "        with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
        "            f.write(\"\"\"{\n",
        "            \"file_format_version\" : \"1.0.0\",\n",
        "            \"ICD\" : {\n",
        "                \"library_path\" : \"libEGL_nvidia.so.0\"\n",
        "            }\n",
        "        }\n",
        "        \"\"\")\n",
        "\n",
        "    # Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
        "    print('Setting environment variable to use GPU rendering:')\n",
        "    %env MUJOCO_GL=egl\n",
        "\n",
        "    # Check if installation was succesful.\n",
        "    try:\n",
        "        print('Checking that the installation succeeded:')\n",
        "        import mujoco\n",
        "        mujoco.MjModel.from_xml_string('<mujoco/>')\n",
        "    except Exception as e:\n",
        "        raise e from RuntimeError(\n",
        "            'Something went wrong during installation. Check the shell output above '\n",
        "            'for more information.\\n'\n",
        "            'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
        "            'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
        "\n",
        "    print('Installation successful.')\n",
        "\n",
        "    # Other imports and helper functions\n",
        "    import time\n",
        "    import itertools\n",
        "    import numpy as np\n",
        "\n",
        "    # Graphics and plotting.\n",
        "    print('Installing mediapy:')\n",
        "    !command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
        "    !pip install -q mediapy\n",
        "    import mediapy as media\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # More legible printing from numpy.\n",
        "    np.set_printoptions(precision=3, suppress=True, linewidth=100)\n",
        "\n",
        "    from IPython.display import clear_output\n",
        "    clear_output()\n"
      ],
      "metadata": {
        "id": "Y07C0IpATP7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install renderlab"
      ],
      "metadata": {
        "id": "o68xk-XVP43Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/Gymnasium-Robotics')\n",
        "import gymnasium as gym\n",
        "import gymnasium_robotics\n",
        "import renderlab as rl\n",
        "\n",
        "ROBOT_PROPRIOCEPTION_KEY = 'observation'\n",
        "ROBOT_XYZ_GOAL_KEY = 'desired_goal'"
      ],
      "metadata": {
        "id": "nUR0dp_RPE7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Downloading Expert File**\n"
      ],
      "metadata": {
        "id": "MqbGVz9z8Zo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/kuanfang/cornell-cs4756-2025sp/raw/main/assignments/A1/programming/expert.pt"
      ],
      "metadata": {
        "id": "CqYWlZSgbroN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Environment and Expert Setup**\n",
        "The environment is held in ```env``` and the expert agent is held in ```actor_network```.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PJczc-qw8rJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def process_inputs(o, g, o_mean, o_std, g_mean, g_std, clip_obs, clip_range):\n",
        "    o_clip = np.clip(o, -clip_obs, clip_obs)\n",
        "    g_clip = np.clip(g, -clip_obs, clip_obs)\n",
        "    o_norm = np.clip((o_clip - o_mean) / (o_std), -clip_range, clip_range)\n",
        "    g_norm = np.clip((g_clip - g_mean) / (g_std), -clip_range, clip_range)\n",
        "    inputs = np.concatenate([o_norm, g_norm])\n",
        "    inputs = torch.tensor(inputs, dtype=torch.float32)\n",
        "    return inputs\n",
        "\n",
        "\n",
        "# define the Expert network\n",
        "class Expert(nn.Module):\n",
        "    def __init__(self, env_params, o_mean, o_std, g_mean, g_std):\n",
        "        super(Expert, self).__init__()\n",
        "        self.max_action = env_params['action_max']\n",
        "        self.fc1 = nn.Linear(env_params[ROBOT_PROPRIOCEPTION_KEY] + env_params[ROBOT_XYZ_GOAL_KEY], 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 256)\n",
        "        self.action_out = nn.Linear(256, env_params['action'])\n",
        "        self.o_mean = o_mean\n",
        "        self.o_std = o_std\n",
        "        self.g_mean = g_mean\n",
        "        self.g_std = g_std\n",
        "        self.clip_obs = 200\n",
        "        self.clip_range = 5\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        actions = self.max_action * torch.tanh(self.action_out(x))\n",
        "\n",
        "        return actions\n",
        "\n",
        "    def get_expert_action(self, prop, g):\n",
        "        inputs = process_inputs(prop, g, self.o_mean, self.o_std, self.g_mean, self.g_std, self.clip_obs, self.clip_range).to(device)\n",
        "        pi = self.forward(inputs)\n",
        "        action = pi.detach().cpu().numpy().squeeze()\n",
        "        return action\n",
        "\n"
      ],
      "metadata": {
        "id": "fcJtRqVSPh31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Notes about Fetch Reach Environment**\n",
        "The environment uses a Fetch Robot, which is a 7-DoF Mobile Manipulator.\n",
        "\n",
        "The task is a *goal-reaching task*:\n",
        "The observation space is a dictionary which contains the robot's proprioception (under the key `ROBOT_PROPRIOCEPTION_KEY`), and the xyz coordinate that the robot's gripper aims to reach (under the key `ROBOT_GOAL_XYZ_KEY`).\n",
        "\n",
        "All agents have functions defined which take the two pieces of data in order to predict an action to take.\n",
        "\n",
        "See https://robotics.farama.org/envs/fetch/reach/ for more details.\n",
        "\n",
        "If the goal is reached, `info['is_success']` will be set to 1, and this is an indication that we should terminate the rollout.\n",
        "\n",
        "The reward (used only for evaluation in this assignment) is -1 per timestep spent in the environment without completing the task, with 50 steps being the limit (so -50 is the worst episode return)."
      ],
      "metadata": {
        "id": "wxFod0nzpU1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)\n",
        "model_path = \"expert.pt\"\n",
        "o_mean, o_std, g_mean, g_std, model = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
        "model\n",
        "# create the environment\n",
        "env = gym.make('FetchReach-v4', render_mode='rgb_array')# get the env param\n",
        "observation, _ = env.reset()\n",
        "# get the environment params\n",
        "env_params = {ROBOT_PROPRIOCEPTION_KEY: observation[ROBOT_PROPRIOCEPTION_KEY].shape[0],\n",
        "                ROBOT_XYZ_GOAL_KEY: observation[ROBOT_XYZ_GOAL_KEY].shape[0],\n",
        "                'action': env.action_space.shape[0],\n",
        "                'action_max': env.action_space.high[0],\n",
        "                }\n",
        "# create the Expert network\n",
        "expert_network = Expert(env_params, o_mean, o_std, g_mean, g_std)\n",
        "expert_network.to(device)\n",
        "expert_network.load_state_dict(model)\n",
        "expert_network.eval()"
      ],
      "metadata": {
        "id": "7_L5P4YnT6Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mujoco_py\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import tqdm\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim import optimizer\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay"
      ],
      "metadata": {
        "id": "d9HT4r0jPZWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the seed to ensure reproducability\n",
        "def reseed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "\n",
        "reseed(42)"
      ],
      "metadata": {
        "id": "QvprulQEPbqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Example Demonstration Rollouts**\n",
        "### **Visualizing the Fetch Reach environment**\n",
        "\n",
        "We have provided functions to visualize the environment and compute rewards on the Fetch Reach environment with random actions or expert actions. Looking through this code will help you get familiarized with the environment, and set you up for the next parts in this assignment.\n",
        "\n",
        "We disabled the visualizations by default since they greatly slow down the evaluation, but feel free to turn it on if you're interested. You'll have to set visualize to True in the cell below and render_mode to 'rgb_array' when setting up the environment. Note that visualization will only be possible if T4 GPU runtime is available."
      ],
      "metadata": {
        "id": "q_qBssuw9BIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_TRAJS = 10\n",
        "visualize = False and torch.cuda.is_available() # set to false in order to disable rendering code\n",
        "if visualize:\n",
        "    env = rl.RenderFrame(env, './output')\n",
        "if visualize:\n",
        "    plt.axis('off')\n",
        "total_reward_random = 0\n",
        "i = 0\n",
        "\n",
        "for k in range(NUM_TRAJS):\n",
        "  done = False\n",
        "  observation, info = env.reset(seed = k)\n",
        "  while not done:\n",
        "    i += 1\n",
        "    if visualize and i%5==0:\n",
        "      ipythondisplay.clear_output(wait=True)\n",
        "      screen = env.render()\n",
        "      plt.imshow(screen)\n",
        "      ipythondisplay.display(plt.gcf())\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated or info['is_success']\n",
        "    total_reward_random += reward\n",
        "    if done:\n",
        "        break\n",
        "total_reward_random /= NUM_TRAJS\n",
        "print(f\"Avg Reward using Random Actions = \", (total_reward_random))"
      ],
      "metadata": {
        "id": "28GY6jO3O3z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_TRAJS = 10\n",
        "visualize = False and torch.cuda.is_available() # set to false in order to disable rendering code\n",
        "if visualize:\n",
        "    env = rl.RenderFrame(env, './output')\n",
        "if visualize:\n",
        "    plt.axis('off')\n",
        "total_reward = 0\n",
        "i = 0\n",
        "\n",
        "for k in range(NUM_TRAJS):\n",
        "  done = False\n",
        "  observation, info = env.reset(seed = k)\n",
        "  while not done:\n",
        "    i += 1\n",
        "    if visualize and i%5==0:\n",
        "      ipythondisplay.clear_output(wait=True)\n",
        "      screen = env.render()\n",
        "      plt.imshow(screen)\n",
        "      ipythondisplay.display(plt.gcf())\n",
        "    action = expert_network.get_expert_action(observation[ROBOT_PROPRIOCEPTION_KEY], observation[ROBOT_XYZ_GOAL_KEY])\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated or info['is_success']\n",
        "    total_reward += reward\n",
        "    if done:\n",
        "        break\n",
        "total_reward /= NUM_TRAJS\n",
        "print(f\"Avg Reward using Expert Actions = \", (total_reward))"
      ],
      "metadata": {
        "id": "3oAYUuYevOZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Approximate expected reward for total reward using random actions: -40 to -50**\n",
        "\n",
        "**Approximate expected reward for total reward using expert actions: -2 to -3**"
      ],
      "metadata": {
        "id": "jW8o41mxo59n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining Learner Agent Class**"
      ],
      "metadata": {
        "id": "UD1ZHLTM9HYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Learner(nn.Module):\n",
        "    def __init__(self, env, env_params, hidden_dim = 256, random_prob=0.0):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(env_params[ROBOT_PROPRIOCEPTION_KEY] + env_params[ROBOT_XYZ_GOAL_KEY], hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc_out = nn.Linear(hidden_dim, env_params['action'])\n",
        "\n",
        "        self.env = env\n",
        "        self.random_prob = random_prob\n",
        "\n",
        "    def forward(self, prop, g):\n",
        "        x = torch.cat([prop, g], dim=-1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        out = F.tanh(self.fc_out(x))\n",
        "        return out\n",
        "\n",
        "    def get_action(self, prop, g):\n",
        "        if np.random.random() < self.random_prob:\n",
        "            return self.env.action_space.sample()\n",
        "        action = self.forward(torch.tensor(prop).unsqueeze(0).to(device).float(), torch.tensor(g).unsqueeze(0).to(device).float())\n",
        "        return np.array(action[0].detach().cpu())"
      ],
      "metadata": {
        "id": "Hhnpo76JY24y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_checkpoint_path(algo):\n",
        "    \"\"\"Return the path to save the best performing model checkpoint.\n",
        "\n",
        "    Parameters:\n",
        "        algo (str)\n",
        "          Indicates which algorithm will be used to train the model\n",
        "\n",
        "    Returns:\n",
        "        checkpoint_path (str)\n",
        "            The path to save the best performing model checkpoint\n",
        "    \"\"\"\n",
        "    if algo == \"bc\":\n",
        "      return 'best_bc_checkpoint.pth'\n",
        "    elif algo == \"dagger\":\n",
        "      return 'best_dagger_checkpoint.pth'\n",
        "    return 'best_model_checkpoint.pth'"
      ],
      "metadata": {
        "id": "v_XStdq5dbbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Collect Expert Demonstrations**"
      ],
      "metadata": {
        "id": "WG0nfNoc9hdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Collecting trajectories (i.e. demonstrations) using the expert policy\n",
        "NUM_TRAJS = 12\n",
        "proprios, goals, actions = [], [], []\n",
        "reseed(1)\n",
        "for traj_num in tqdm(range(NUM_TRAJS), position=0, leave=True):\n",
        "    done = False\n",
        "    observation, _ = env.reset(seed = traj_num)\n",
        "    while not done:\n",
        "        with torch.no_grad():\n",
        "            action = expert_network.get_expert_action(observation[ROBOT_PROPRIOCEPTION_KEY], observation[ROBOT_XYZ_GOAL_KEY])\n",
        "            proprios.append(observation[ROBOT_PROPRIOCEPTION_KEY])\n",
        "            goals.append(observation[ROBOT_XYZ_GOAL_KEY])\n",
        "            action[-1] = 0\n",
        "            actions.append(action)\n",
        "            observation, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated or info['is_success']\n",
        "        if done:\n",
        "            break\n",
        "train_proprios = proprios[:int(0.5*len(proprios))]\n",
        "train_goals = goals[:int(0.5*len(proprios))]\n",
        "train_actions = actions[:int(0.5*len(actions))]\n",
        "validation_proprios = proprios[int(0.5*len(proprios)):]\n",
        "validation_goals = goals[int(0.5*len(proprios)):]\n",
        "validation_actions = actions[int(0.5*len(actions)):]"
      ],
      "metadata": {
        "id": "EmomR2K7ddeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 1: (CS 4756/5756) **Implement and train BC Agent**"
      ],
      "metadata": {
        "id": "q-C_Gzco9l9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To begin, fill in the implementation for the training loop function in **`bc_train`**. We provide the loss function and optimizer already, just iterate through your dataloader and return the updated policy!\n",
        "\n",
        "You'll also measure the validation loss at each iteration to check for overfitting.\n",
        "\n",
        "Once you finish the training loop implementation, it is now time to build up your agents! **Behavior cloning (BC)** is the simplest imitation learning algorithm, where we perform supervised learning on the given (offline) expert dataset. We either do this via log-likelihood maximization (cross-entropy minimization) in the discrete action case, or mean-squared error minimization (can also do MLE) in the continuous control setting.\n",
        "\n",
        "If implemented correctly, training your BC model should take at most 1 minute."
      ],
      "metadata": {
        "id": "RZCo8xmxrGjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import tqdm\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim import optimizer\n",
        "\n",
        "def evaluate(env, learner):\n",
        "    NUM_TRAJS = 20\n",
        "    total_learner_reward = 0\n",
        "    for i in range(NUM_TRAJS):\n",
        "        done = False\n",
        "        observation, _ = env.reset(seed = i)\n",
        "        while not done:\n",
        "            action = learner.get_action(observation[ROBOT_PROPRIOCEPTION_KEY], observation[ROBOT_XYZ_GOAL_KEY])\n",
        "            observation, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated or info['is_success']\n",
        "            total_learner_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "    return total_learner_reward / NUM_TRAJS\n",
        "\n",
        "def bc_train(learner, proprios, goals, actions, validation_proprios, validation_goals, validation_acts, checkpoint_path, num_epochs=100):\n",
        "    \"\"\"Train function for learning a new policy using BC.\n",
        "\n",
        "    Parameters:\n",
        "        learner (Learner)\n",
        "            A Learner object (policy)\n",
        "        proprios (list of torch.tensor)\n",
        "            A list of pytorch arrays of shape (N, 10, )\n",
        "        goals (list of torch.tensor)\n",
        "            A list of pytorch arrays of shape (N, 3, )\n",
        "        actions (list of torch.tensor)\n",
        "            A list of pytorch arrays of shape (N, 4, )\n",
        "        checkpoint_path (str)\n",
        "            The path to save the best performing checkpoint\n",
        "        num_epochs (int)\n",
        "            Number of epochs to run the train function for\n",
        "\n",
        "    Returns:\n",
        "        learner (Learner)\n",
        "            A Learner object (policy)\n",
        "    \"\"\"\n",
        "    best_loss = float('inf')\n",
        "    best_model_state = None\n",
        "\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(learner.parameters(), lr=3e-4)\n",
        "    dataset = TensorDataset(proprios, goals, actions) # Create your dataset\n",
        "    dataloader = DataLoader(dataset, batch_size=256, shuffle=True) # Create your dataloader\n",
        "\n",
        "    # TODO: Complete the training loop here ###\n",
        "    loss = float('inf')\n",
        "    validation_losses = []\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        epoch_loss = 0\n",
        "        # BEGIN SOLUTION\n",
        "\n",
        "        # Calculate Validation Loss (remember to use \"with torch.no_grad():\")\n",
        "\n",
        "        # END SOLUTION\n",
        "\n",
        "        # Saving model state if current loss is less than best loss\n",
        "        if epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            # Save the best performing checkpoint\n",
        "            torch.save(learner.state_dict().copy(), checkpoint_path)\n",
        "    # Save graph if not running DAGGER:\n",
        "    if checkpoint_path == 'best_bc_checkpoint.pth':\n",
        "        plt.plot(np.arange(0, num_epochs), [t.cpu().numpy() if isinstance(t, torch.Tensor) else t for t in validation_losses])\n",
        "        plt.title(\"Validation Loss vs. Iteration\")\n",
        "        plt.xlabel(\"Iteration\")\n",
        "        plt.ylabel(\"Validation Loss\")\n",
        "        plt.savefig(\"BC_validation_loss.png\")\n",
        "\n",
        "    return learner"
      ],
      "metadata": {
        "id": "HZemVLJsl7Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting graph should show a clear downward trend without curving back up at the right side. If the validation loss increases at the end, it indicates overfitting."
      ],
      "metadata": {
        "id": "1GPQkFimrgnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bc_learner = Learner(env, env_params)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "bc_learner.to(device)\n",
        "checkpoint_path = get_checkpoint_path(\"bc\")\n",
        "reseed(2)\n",
        "bc_train(\n",
        "    bc_learner,\n",
        "    torch.tensor(train_proprios).to(device).float(),\n",
        "    torch.tensor(train_goals).to(device).float(),\n",
        "    torch.tensor(train_actions).to(device).float(),\n",
        "    torch.tensor(validation_proprios).to(device).float(),\n",
        "    torch.tensor(validation_goals).to(device).float(),\n",
        "    torch.tensor(validation_actions).to(device).float(),\n",
        "    checkpoint_path,\n",
        "    num_epochs = 1500\n",
        ")"
      ],
      "metadata": {
        "id": "bTVYj5MJmGBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the BC Agent's Performance\n",
        "**Approximate expected reward for BC learner: -10 to -15**"
      ],
      "metadata": {
        "id": "TgUQ8gxorl01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bc_learner = Learner(env, env_params)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "bc_learner.to(device)\n",
        "checkpoint_path = get_checkpoint_path(\"bc\")\n",
        "bc_learner.load_state_dict(torch.load(checkpoint_path))\n",
        "bc_learner.eval()\n",
        "\n",
        "print(\"Avg Reward using Learner Actions = \", (evaluate(env, bc_learner)))\n",
        "print(\"Avg Reward using Expert Actions = \", (total_reward))\n",
        "\n"
      ],
      "metadata": {
        "id": "xLTLV-wTn9_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 2: (CS 5756 Only) **Implement and train DAgger Agent**"
      ],
      "metadata": {
        "id": "FcULu9NS91A7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset aggregation (DAgger)** is a fundamentally interactive algorithm, where we can query the expert any time we want to get information about how to proceed. This allows for significantly more freedom for the learner, as it can ask the expert anywhere and not be limited by the dataset that it is given to learn from.\n",
        "\n",
        "**Can we overcome suboptimal actions with DAgger?** Fundamentally, this algorithm allows the learner to recover from bad states and should lead to much better performance than simply behavior cloning a fixed set of expert demonstrations. For this portion of the assignment, you will interact with the environment using the learner policy with random actions. You will do so in **`interact`**.\n",
        "\n",
        "The DAgger policy will be initialized with the already learned BC policy and your dataset with the already collected expert demonstrations for BC.\n",
        "\n",
        "Training should take no longer than 2 minutes if implemented correctly."
      ],
      "metadata": {
        "id": "jQuFeDwmsJnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interact(env, learner, expert_network, proprios, goals, actions, validation_proprios, validation_goals, validation_actions, checkpoint_path, seed, num_epochs=100, horizon=50):\n",
        "    \"\"\"Interact with the environment and update the learner policy using DAgger.\n",
        "\n",
        "    This function interacts with the given Gym environment and aggregates to\n",
        "    the BC dataset by querying the expert.\n",
        "\n",
        "    Parameters:\n",
        "        env (Env)\n",
        "            The gym environment (in this case, the Hopper gym environment)\n",
        "        learner (Learner)\n",
        "            A Learner object (policy)\n",
        "        expert_network (Expert)\n",
        "            An Expert object (expert policy)\n",
        "        proprios (list of torch.tensor)\n",
        "            A list of pytorch arrays of shape (N, 10, )\n",
        "        goals (list of torch.tensor)\n",
        "            A list of pytorch arrays of shape (N, 3, )\n",
        "        actions (list of torch.tensor)\n",
        "            A list of pytorch arrays of shape (N, 4, )\n",
        "        checkpoint_path (str)\n",
        "            The path to save the best performing model checkpoint\n",
        "        seed (int)\n",
        "            The seed to use for the environment\n",
        "        num_epochs (int)\n",
        "            Number of epochs to run the train function for\n",
        "    \"\"\"\n",
        "    # Interact with the environment and aggregate your BC Dataset by querying the expert\n",
        "    NUM_INTERACTIONS = 9\n",
        "    best_reward = float('-inf')\n",
        "    best_model_state = None\n",
        "    for episode in range(NUM_INTERACTIONS):\n",
        "        # Aggregate 2 trajectories per interaction\n",
        "        for _ in range(2):\n",
        "            done = False\n",
        "            observation, _ = env.reset(seed = episode)\n",
        "            for _ in range(horizon):\n",
        "                # TODO: Implement Fetch Reach environment interaction and dataset aggregation here\n",
        "                #BEGIN SOLUTION\n",
        "\n",
        "\n",
        "                #END SOLUTION\n",
        "                if done:\n",
        "                    break\n",
        "        bc_train(learner,\n",
        "                 torch.tensor(proprios).to(device).float(),\n",
        "                torch.tensor(goals).to(device).float(),\n",
        "                torch.tensor(actions).to(device).float(),\n",
        "                torch.tensor(validation_proprios).to(device).float(),\n",
        "                torch.tensor(validation_goals).to(device).float(),\n",
        "                torch.tensor(validation_actions).to(device).float(),\n",
        "                 \"DAgger_Interaction_{}.pth\".format(episode), num_epochs)\n",
        "        reward = evaluate(env, learner)\n",
        "        print(f\"After interaction {episode}, reward = {reward}\")\n",
        "        # Saving model state if current reward is greater than best reward\n",
        "        if reward > best_reward:\n",
        "            best_reward = reward\n",
        "            # Save the best performing checkpoint\n",
        "            torch.save(learner.state_dict().copy(), checkpoint_path)\n"
      ],
      "metadata": {
        "id": "ysZJk_Cawvjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Approximate expected reward for DAgger learner: -4 to -8**"
      ],
      "metadata": {
        "id": "8WMuceBCwWro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize DAgger Agent with BC learner's weights\n",
        "dagger_learner = Learner(env, env_params)\n",
        "checkpoint_path = get_checkpoint_path(\"bc\")\n",
        "dagger_learner.load_state_dict(torch.load(checkpoint_path))\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "dagger_learner.to(device)"
      ],
      "metadata": {
        "id": "tmUVp6iRzYaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = get_checkpoint_path(\"dagger\")\n",
        "seed = 2\n",
        "reseed(seed)\n",
        "interact(env, dagger_learner, expert_network, torch.tensor(train_proprios).to(device), torch.tensor(train_goals).to(device), torch.tensor(train_actions).to(device), torch.tensor(validation_proprios).to(device), torch.tensor(validation_goals), torch.tensor(validation_actions).to(device), checkpoint_path, seed, num_epochs = 500)"
      ],
      "metadata": {
        "id": "eQTRN4i9zhnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dagger_learner = Learner(env, env_params)\n",
        "checkpoint_path = get_checkpoint_path(\"dagger\")\n",
        "dagger_learner.load_state_dict(torch.load(checkpoint_path))\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "dagger_learner.to(device)\n",
        "dagger_learner.eval()\n",
        "\n",
        "print(\"Avg Reward using DAgger Learner Actions = \", (evaluate(env, dagger_learner)))\n",
        "print(\"Avg Reward using Expert Actions = \", (total_reward))"
      ],
      "metadata": {
        "id": "X1x0FNAooLw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E1877kA6tMaj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}